import argparse
import json
import os
import sqlite3
import bz2
from pathlib import Path
from typing import Optional, List, Dict, Any, Tuple, Set, Iterator
import numpy as np
import torch
import yaml
from huggingface_hub import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
import uuid
import logging
import shutil
import tempfile
import torch.nn as nn
import torch.nn.functional as F
from collections import Counter
from dataclasses import dataclass
from scipy.stats import rankdata
import math

"""
Next Steps for Implementation and Expansion:
1. Integrating XLSTM and RL Feedback Loops:
2. RL-Driven Metric Exploration:
3. Incorporating Diversity via RL:
        In addition to minimizing rank correlation, RL can also reward diversity based on the relationship between tensors across different layers, helping to prevent overfitting and ensure generalization.
        Use the diversity threshold in select_top_tensors() to make sure the RL agent doesn't pick similar tensors (or tensors that overfit to specific dataset properties). This would ensure that the fine-tuned model incorporates a broader range of knowledge.
4. Incorporating Larger Knowledge Base via Embeddings:
        Use the conversation signature embeddings to augment the training data for tensor optimization. By enriching the signature information with knowledge base embeddings, you can make the model more context-aware during the ranking process.This could be achieved by combining the term frequencies from the conversation signature with embeddings from a knowledge base to refine the tensor selection process.
5. Persistent Model Updates:
        Implement a continuous fine-tuning loop that uses the ranked tensors and RL guidance to improve the composite model iteratively. By recurrent updates, the system can build a persistent sense of self (knowledge evolution), which gets enriched as itâ€™s exposed to more datasets.
"""

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def save_tensor_to_tempfile(tensor: torch.Tensor) -> str:
    with tempfile.NamedTemporaryFile(delete=False) as temp_file:
        tensor_path = temp_file.name
        torch.save(tensor, tensor_path)  # Save the tensor to the temporary file
    return tensor_path


def load_tensor_from_tempfile(tensor_path: str) -> torch.Tensor:
    return torch.load(tensor_path)


class ModelDatabase:
    def _init_database(self) -> None:
        """Now with more model reconstruction juice!"""
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript(
                """
                -- Keep track of base models and their config
                CREATE TABLE IF NOT EXISTS base_models (
                    base_model_id TEXT PRIMARY KEY,
                    model_name TEXT UNIQUE NOT NULL,
                    config_json TEXT NOT NULL,
                    tokenizer_json TEXT,
                    tokenizer_config_json TEXT,
                    special_tokens_map_json TEXT,
                    added_tokens_json TEXT
                );

                -- Track derived models and their tensor composition
                CREATE TABLE IF NOT EXISTS derived_models (
                    model_id TEXT PRIMARY KEY,
                    model_name TEXT UNIQUE NOT NULL,
                    base_model_id TEXT NOT NULL,
                    creation_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(base_model_id) REFERENCES base_models(base_model_id)
                );

                -- Store tensors with their source info
                CREATE TABLE IF NOT EXISTS tensors (
                    tensor_id TEXT PRIMARY KEY,
                    model_id TEXT NOT NULL,
                    tensor_path TEXT NOT NULL,     -- Full path in model
                    tensor_data BLOB NOT NULL,
                    tensor_shape TEXT NOT NULL,    
                    tensor_dtype TEXT NOT NULL,
                    source_model_id TEXT NOT NULL, -- Track where tensor came from
                    FOREIGN KEY(model_id) REFERENCES derived_models(model_id),
                    UNIQUE(model_id, tensor_path)
                );

                -- Define tensor loading order for model assembly
                CREATE TABLE IF NOT EXISTS tensor_loading_order (
                    model_id TEXT NOT NULL,
                    tensor_id TEXT NOT NULL,
                    load_order INTEGER NOT NULL,
                    FOREIGN KEY(model_id) REFERENCES derived_models(model_id),
                    FOREIGN KEY(tensor_id) REFERENCES tensors(tensor_id),
                    UNIQUE(model_id, load_order),
                    UNIQUE(model_id, tensor_id)
                );

                -- Keep the metrics tables
                CREATE TABLE IF NOT EXISTS tensor_metrics (
                    tensor_id TEXT NOT NULL,
                    metric_name TEXT NOT NULL,
                    metric_value REAL NOT NULL,
                    FOREIGN KEY(tensor_id) REFERENCES tensors(tensor_id),
                    UNIQUE(tensor_id, metric_name)
                );

                CREATE TABLE IF NOT EXISTS cross_tensor_metrics (
                    source_tensor_id TEXT NOT NULL,
                    target_tensor_id TEXT NOT NULL,
                    metric_name TEXT NOT NULL,
                    metric_value REAL NOT NULL,
                    FOREIGN KEY(source_tensor_id) REFERENCES tensors(tensor_id),
                    FOREIGN KEY(target_tensor_id) REFERENCES tensors(tensor_id),
                    UNIQUE(source_tensor_id, target_tensor_id, metric_name)
                );
                """
            )

    def store_base_model(
        self, model_name: str, config: dict, tokenizer_files: dict
    ) -> str:
        """Stores base model configuration and tokenizer files."""
        base_model_id = str(uuid.uuid4())
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO base_models (
                    base_model_id, model_name, config_json,
                    tokenizer_json, tokenizer_config_json,
                    special_tokens_map_json, added_tokens_json
                ) VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    base_model_id,
                    model_name,
                    json.dumps(config),
                    tokenizer_files.get("tokenizer_json"),
                    tokenizer_files.get("tokenizer_config_json"),
                    tokenizer_files.get("special_tokens_map_json"),
                    tokenizer_files.get("added_tokens_json"),
                ),
            )
            conn.commit()
        return base_model_id

    def create_derived_model(
        self, model_name: str, base_model_name: str, tensor_specs: List[Dict[str, Any]]
    ) -> str:
        """Creates a new model from specified tensors."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT base_model_id FROM base_models WHERE model_name = ?",
                (base_model_name,),
            )
            result = cursor.fetchone()
            if not result:
                raise ValueError(f"Base model {base_model_name} not found")
            base_model_id = result[0]
            model_id = str(uuid.uuid4())
            cursor.execute(
                """
                INSERT INTO derived_models (model_id, model_name, base_model_id)
                VALUES (?, ?, ?)
                """,
                (model_id, model_name, base_model_id),
            )

            for order, spec in enumerate(tensor_specs):
                tensor_id = spec["tensor_id"]
                cursor.execute(
                    """
                    INSERT INTO tensor_loading_order (model_id, tensor_id, load_order)
                    VALUES (?, ?, ?)
                    """,
                    (model_id, tensor_id, order),
                )

            conn.commit()
            return model_id

    def reconstruct_model(
        self, model_name: str, device: str = "cpu"
    ) -> Tuple[Any, Any]:
        """Reconstructs a working model from stored tensors."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                SELECT dm.model_id, bm.base_model_id, bm.config_json,
                       bm.tokenizer_json, bm.tokenizer_config_json,
                       bm.special_tokens_map_json, bm.added_tokens_json
                FROM derived_models dm
                JOIN base_models bm ON dm.base_model_id = bm.base_model_id
                WHERE dm.model_name = ?
                """,
                (model_name,),
            )
            result = cursor.fetchone()
            if not result:
                raise ValueError(f"Model {model_name} not found")

            model_id, base_model_id, config_json = result[0:3]
            tokenizer_files = {
                "tokenizer_json": result[3],
                "tokenizer_config_json": result[4],
                "special_tokens_map_json": result[5],
                "added_tokens_json": result[6],
            }

            with tempfile.TemporaryDirectory() as temp_dir:
                temp_dir = Path(temp_dir)
                config = json.loads(config_json)
                with open(temp_dir / "config.json", "w") as f:
                    json.dump(config, f)

                for filename, content in tokenizer_files.items():
                    if content:
                        with open(
                            temp_dir / f"{filename.replace('_json', '.json')}", "w"
                        ) as f:
                            f.write(content)

                cursor.execute(
                    """
                    SELECT t.tensor_path, t.tensor_data, t.tensor_shape, t.tensor_dtype
                    FROM tensor_loading_order tlo
                    JOIN tensors t ON tlo.tensor_id = t.tensor_id
                    WHERE tlo.model_id = ?
                    ORDER BY tlo.load_order
                    """,
                    (model_id,),
                )

                model = AutoModelForCausalLM.from_config(config)
                model = model.to(device)
                for path, data, shape, dtype in cursor.fetchall():
                    tensor = (
                        torch.frombuffer(
                            data, dtype=getattr(torch, dtype.split(".")[-1])
                        )
                        .reshape(json.loads(shape))
                        .clone()
                        .to(device)
                    )
                    module_path, param_name = path.rsplit(".", 1)
                    if module_path:
                        module = model.get_submodule(module_path)
                    else:
                        module = model
                    setattr(module, param_name, nn.Parameter(tensor))

                tokenizer = AutoTokenizer.from_pretrained(temp_dir)
                return model, tokenizer

    def validate_model_integrity(
        self, model_name: str, required_tensors: Optional[Set[str]] = None
    ) -> Dict[str, Any]:
        """Validates that all necessary tensors are present and correctly ordered."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # Get all model tensors
            cursor.execute(
                """
                SELECT t.tensor_path, t.tensor_shape, tlo.load_order
                FROM tensor_loading_order tlo
                JOIN tensors t ON tlo.tensor_id = t.tensor_id
                JOIN derived_models dm ON tlo.model_id = dm.model_id
                WHERE dm.model_name = ?
                ORDER BY tlo.load_order
                """,
                (model_name,),
            )

            tensors = cursor.fetchall()
            if not tensors:
                raise ValueError(f"No tensors found for model {model_name}")

            # Validate tensor presence
            tensor_paths = {t[0] for t in tensors}
            if required_tensors and not required_tensors.issubset(tensor_paths):
                missing = required_tensors - tensor_paths
                raise ValueError(f"Missing required tensors: {missing}")

            # Check for loading order gaps
            orders = [t[2] for t in tensors]
            if set(orders) != set(range(len(orders))):
                raise ValueError("Gaps detected in tensor loading order")

            return {
                "tensor_count": len(tensors),
                "tensors": {
                    path: {"shape": json.loads(shape), "load_order": order}
                    for path, shape, order in tensors
                },
            }


class ModelLoader:
    """Downloads models and extracts their tensor goodness."""

    def __init__(self, models_dir: str, database: ModelDatabase):
        self.models_dir = models_dir
        self.database = database  # We need this for the tensor extraction magic
        Path(models_dir).mkdir(parents=True, exist_ok=True)

    def ensure_model_files(self, model_name: str) -> str:
        """Grabs model files, but now also extracts tokenizer configs."""
        local_path = Path(self.models_dir) / model_name.replace("/", "_")

        if local_path.exists() and (local_path / "config.json").exists():
            return str(local_path)

        logger.info(f"Yoinking {model_name} from the hub...")
        try:
            snapshot_download(
                repo_id=model_name,
                local_dir=local_path,
                local_dir_use_symlinks=False,
                revision="main",
            )

            # Snag all the tokenizer goodies
            tokenizer_files = {}
            for filename in [
                "tokenizer.json",
                "tokenizer_config.json",
                "special_tokens_map.json",
                "added_tokens.json",
            ]:
                file_path = local_path / filename
                if file_path.exists():
                    tokenizer_files[f"{filename.split('.')[0]}_json"] = (
                        file_path.read_text()
                    )

            # Store base model info right away
            with open(local_path / "config.json") as f:
                config = json.load(f)

            self.database.store_base_model(
                model_name=model_name, config=config, tokenizer_files=tokenizer_files
            )

            return str(local_path)
        except Exception as e:
            logger.error(f"Failed to download {model_name}: {e}")
            raise

    def load_model(
        self, model_path: str, extract_tensors: bool = True, device: str = "cpu"
    ) -> Tuple[AutoModelForCausalLM, Optional[Dict[str, torch.Tensor]]]:
        """Loads a model and optionally extracts all its juicy tensors."""
        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                device_map={"": device},
            )

            if not extract_tensors:
                return model, None

            # Extract ALL the tensors!
            tensor_map = {}
            for name, param in model.named_parameters():
                if param.requires_grad:  # Only grab the trainable ones
                    tensor_map[name] = param.data.clone().cpu()

            return model, tensor_map

        except Exception as e:
            logger.error(f"Failed to load model from {model_path}: {e}")
            raise

    def extract_tensor_specs(
        self, source_model: str, tensor_paths: List[str]
    ) -> List[Dict[str, Any]]:
        """Gets specs for requested tensors from a source model."""
        specs = []

        with sqlite3.connect(self.database.db_path) as conn:
            cursor = conn.cursor()

            for path in tensor_paths:
                cursor.execute(
                    """
                    SELECT tensor_id, tensor_shape, tensor_dtype
                    FROM tensors t
                    JOIN derived_models dm ON t.model_id = dm.model_id
                    WHERE dm.model_name = ? AND t.tensor_path = ?
                    """,
                    (source_model, path),
                )

                result = cursor.fetchone()
                if not result:
                    raise ValueError(f"Tensor {path} not found in model {source_model}")

                specs.append(
                    {
                        "tensor_id": result[0],
                        "path": path,
                        "shape": json.loads(result[1]),
                        "dtype": result[2],
                    }
                )

        return specs

    @staticmethod
    def validate_tensor_compatibility(
        base_config: dict, tensor_specs: List[Dict[str, Any]]
    ) -> bool:
        """Makes sure tensors will actually fit in the model architecture."""
        try:
            # TODO: Add architecture-specific validation here
            return True

        except Exception as e:
            logger.error(f"Tensor validation failed: {e}")
            return False


class TensorAnalyzer:
    """Analyzes tensors without caring about their supposed purpose in life."""

    def __init__(self, device: torch.device):
        self.device = device
        self.current_cache = None
        self.cross_cache = None
        self._setup_metrics()

    def _setup_metrics(self):
        """Define our mathematical arsenal."""
        self.metrics = {
            "snr": self._calculate_snr,
            "svd_skewness": self._calculate_svd_skewness,
            "stable_rank": self._calculate_stable_rank,
            "normalized_effective_rank": self._calculate_normalized_effective_rank,
            "weight_spectral_norm": self._calculate_spectral_norm,
            "weight_kurtosis": self._calculate_kurtosis,
            "weight_skewness": self._calculate_skewness,
            "weight_sparsity": self._calculate_sparsity,
            "weight_entropy": self._calculate_entropy,
            "outlier_influence": self._calculate_outliers,
            "weight_clustering": self._calculate_clustering,
            "mode_collapse": self._calculate_mode_collapse,
            "zipf_deviation": self._calculate_zipf_deviation,
            "bzip2_compression": self._calculate_compression,
            "weight_memorization": self._calculate_memorization,
            "lyapunov_estimate": self._calculate_lyapunov,
            "permutation_entropy": self._calculate_permutation_entropy,
            "weight_temperature": self._calculate_temperature,
            "phase_coherence": self._calculate_phase_coherence,
            "wasserstein": self._calculate_wasserstein,
            "phase_space": self._calculate_phase_space,
            "persistence": self._calculate_persistence,
        }

        self.cross_metrics = {
            "mutual_information": self._calculate_mutual_info,
            "cosine_similarity": self._calculate_cosine_sim,
            "cucconi": self._calculate_cucconi,
            "cvd": self._calculate_cvd,
            "earth_mover": self._calculate_earth_mover,
            "distribution_overlap": self._calculate_distribution_overlap,
        }

    def _build_cache(self, tensor: torch.Tensor) -> dict:
        """Precompute everything we might need."""
        cache = {}
        cache["tensor"] = self._normalize_tensor(tensor.to(self.device))
        cache["shape"] = tensor.shape
        cache["flat"] = cache["tensor"].flatten()
        cache["sorted"] = torch.sort(cache["flat"])[0]
        cache["numel"] = cache["flat"].numel()
        cache["mean"] = torch.mean(cache["flat"])
        cache["std"] = torch.std(cache["flat"])
        cache["var"] = torch.var(cache["flat"])
        q_vals = torch.tensor([0.25, 0.75], device=self.device)
        cache["quartiles"] = torch.quantile(cache["flat"], q_vals)
        cache["iqr"] = cache["quartiles"][1] - cache["quartiles"][0]
        cache["hist"] = self._compute_histogram(cache["flat"])
        cache["svd"] = torch.linalg.svdvals(cache["tensor"])
        cache["rank"] = torch.linalg.matrix_rank(cache["tensor"])
        cache["norm"] = torch.linalg.norm(cache["tensor"])
        cache["zero_mask"] = torch.abs(cache["tensor"]) < 1e-5
        cache["sparsity"] = cache["zero_mask"].float().mean()
        cache["ranks"] = torch.argsort(cache["flat"].float()).argsort().float()
        cache["angles"] = torch.angle(cache["flat"][1:] + 1j * cache["flat"][:-1])

        return cache

    @torch.inference_mode()
    def analyze(self, tensor: torch.Tensor) -> Dict[str, float]:
        """Analyze a single tensor."""
        self.current_cache = self._build_cache(tensor)
        results = {}
        for name, func in self.metrics.items():
            try:
                results[name] = func()
            except Exception as e:
                logger.error(f"Error calculating {name}: {e}")
                results[name] = float("nan")
        results["rank_compression"] = (
            results["normalized_effective_rank"] * results["bzip2_compression"]
        )
        results["rank_stability"] = (
            results["normalized_effective_rank"] * results["stable_rank"]
        )
        return results

    def compare(
        self, tensor_a: torch.Tensor, tensor_b: torch.Tensor
    ) -> Dict[str, float]:
        """Compare two tensors."""
        self.current_cache = self._build_cache(tensor_a)
        self.cross_cache = self._build_cache(tensor_b)
        results = {}
        for name, func in self.cross_metrics.items():
            try:
                results[name] = func()
            except Exception as e:
                logger.error(f"Error calculating {name}: {e}")
                results[name] = float("nan")

        return results

    def _normalize_tensor(self, tensor: torch.Tensor) -> torch.Tensor:
        """Normalize while preserving signs."""
        signs = tensor.sign()
        tensor = tensor.abs()
        tensor /= tensor.max().clamp(min=1e-8)
        return tensor * signs

    def _compute_histogram(
        self, values: torch.Tensor, bins: int = None
    ) -> torch.Tensor:
        """Adaptive histogram computation."""
        if bins is None:
            bins = int(values.numel() ** 0.5)
            bins = max(bins, 2)

        edges = torch.quantile(
            values, torch.linspace(0, 1, bins + 1, device=self.device)
        )
        hist = torch.histogram(values, edges)
        return hist.hist / hist.hist.sum()

    def _marchenko_pastur_threshold(self, sigma: float, n: int, m: int) -> float:
        """Compute M-P threshold for singular values."""
        beta = n / m
        return sigma * (1 + torch.sqrt(torch.tensor(beta)))

    def _calculate_snr(self) -> float:
        """Signal-to-noise ratio using M-P threshold."""
        S = self.current_cache["svd"]
        max_sv = S[0]
        sigma = S[-len(S) // 4 :].std()  # Use last quartile for noise estimation
        n, m = self.current_cache["shape"][-2:]
        threshold = self._marchenko_pastur_threshold(sigma, n, m)
        signal = torch.sum(S[S > threshold])
        noise = torch.sum(S[S <= threshold])
        snr = signal / max(noise, 1e-10)
        return float(snr / max_sv)

    def _calculate_svd_skewness(self) -> float:
        """Skewness of singular value distribution."""
        S = self.current_cache["svd"]
        return float(1 - (torch.mean(S) / S[0]))

    def _calculate_stable_rank(self) -> float:
        """Stable rank from SVD."""
        S = self.current_cache["svd"]
        return float(torch.sum(S**2) / S[0] ** 2)

    def _calculate_normalized_effective_rank(self) -> float:
        """Normalized effective rank."""
        S = self.current_cache["svd"]
        S_norm = S / S.sum()
        effective_rank = torch.exp(-torch.sum(S_norm * torch.log(S_norm)))
        return float(effective_rank / self.current_cache["rank"])

    def _calculate_spectral_norm(self) -> float:
        """Largest singular value."""
        return float(self.current_cache["svd"][0])

    def _calculate_frobenius_norm(self) -> float:
        """Direct from cache."""
        return float(self.current_cache["norm"])

    def _calculate_kurtosis(self) -> float:
        """Direct from flat values."""
        return float(torch.kurtosis(self.current_cache["flat"]))

    def _calculate_skewness(self) -> float:
        """Direct from flat values."""
        return float(torch.skew(self.current_cache["flat"]))

    def _calculate_sparsity(self) -> float:
        """Direct from cache."""
        return float(self.current_cache["sparsity"])

    def _calculate_entropy(self) -> float:
        """From cached histogram."""
        hist = self.current_cache["hist"]
        return float(-torch.sum(hist * torch.log2(hist + 1e-12)))

    def _calculate_outliers(self) -> float:
        """Using cached quartiles."""
        flat = self.current_cache["flat"]
        q75 = self.current_cache["quartiles"][1]
        iqr = self.current_cache["iqr"]
        threshold = q75 + 1.5 * iqr
        outliers = flat[flat > threshold]
        return (
            float(outliers.sum().abs() / flat.abs().sum()) if len(outliers) > 0 else 0.0
        )

    def _calculate_clustering(self) -> float:
        """Peak detection in histogram."""
        hist = self.current_cache["hist"]
        peaks = ((hist[1:-1] > hist[:-2]) & (hist[1:-1] > hist[2:])).sum()
        return float(peaks) / (len(hist) // 2)

    def _calculate_mode_collapse(self) -> float:
        """Using cached SVD."""
        S = self.current_cache["svd"]
        squared_sv = S**2
        pr = squared_sv.sum() ** 2 / (squared_sv**2).sum()
        return float(pr / len(S))

    def _calculate_zipf_deviation(self) -> float:
        """Compare to ideal Zipf distribution."""
        sorted_vals = self.current_cache["sorted"]
        ranks = torch.arange(
            1, len(sorted_vals) + 1, device=self.device, dtype=torch.float32
        )
        ideal_zipf = 1 / ranks
        ideal_zipf = ideal_zipf / ideal_zipf.sum()
        actual_dist = sorted_vals / sorted_vals.sum()
        kl_div = torch.sum(actual_dist * torch.log2(actual_dist / ideal_zipf + 1e-10))
        return float(1 / (1 + kl_div))

    def _calculate_compression(self) -> float:
        """bzip2 compression ratio."""
        tensor_bytes = self.current_cache["tensor"].cpu().numpy().tobytes()
        return len(bz2.compress(tensor_bytes)) / len(tensor_bytes)

    def _calculate_memorization(self) -> float:
        """Based on compression ratio."""
        return 1 - self._calculate_compression()

    def _calculate_lyapunov(self) -> float:
        """Estimate largest Lyapunov exponent."""
        diffs = torch.diff(self.current_cache["flat"])
        div_rates = torch.log(torch.abs(diffs) + 1e-10)
        positive_rates = div_rates[div_rates > 0]
        return float(positive_rates.mean()) if len(positive_rates) > 0 else 0.0

    def _calculate_permutation_entropy(self) -> float:
        """Ordinal pattern complexity."""
        flat = self.current_cache["flat"]
        n = min(4, len(flat) - 1)
        patterns = []
        for i in range(len(flat) - n):
            pattern = flat[i : i + n]
            perm = torch.argsort(pattern)
            patterns.append(tuple(perm.tolist()))

        pattern_counts = Counter(patterns)
        probs = torch.tensor(
            [count / len(patterns) for count in pattern_counts.values()]
        )
        entropy = -torch.sum(probs * torch.log2(probs + 1e-10))
        return float(entropy / np.log2(math.factorial(n)))

    def _calculate_temperature(self) -> float:
        """Normalized variance-based temperature."""
        var = self.current_cache["var"]
        max_val = self.current_cache["sorted"][-1]
        temp = var / (max_val**2 + 1e-10)
        return float(2 / (1 + torch.exp(-5 * temp)) - 1)

    def _calculate_phase_coherence(self) -> float:
        """Using cached angles."""
        angles = self.current_cache["angles"]
        return float(torch.abs(torch.mean(torch.exp(1j * angles))))

    def _calculate_wasserstein(self) -> float:
        """Distance to normal distribution."""
        sorted_tensor = self.current_cache["sorted"]
        normal_samples = torch.sort(torch.randn_like(sorted_tensor))[0]
        return float(torch.abs(sorted_tensor - normal_samples).mean())

    def _calculate_phase_space(self) -> float:
        """Using delay embedding."""
        x = (
            self.current_cache["flat"] - self.current_cache["mean"]
        ) / self.current_cache["std"]
        tau = max(1, len(x) // 100)
        x1 = x[: -2 * tau]
        x2 = x[tau:-tau]
        x3 = x[2 * tau :]
        points = torch.stack([x1, x2, x3])
        dists = torch.cdist(points.T, points.T)
        return float(torch.exp(-dists).mean())

    def _calculate_persistence(self) -> float:
        """Topological persistence from histogram."""
        hist = self.current_cache["hist"]
        peaks = []
        valleys = []
        for i in range(1, len(hist) - 1):
            if hist[i - 1] < hist[i] > hist[i + 1]:
                peaks.append((i, hist[i]))
            elif hist[i - 1] > hist[i] < hist[i + 1]:
                valleys.append((i, hist[i]))
        persistence = 0.0
        if peaks and valleys:
            for peak_idx, peak_val in peaks:
                closest_valley = min(valleys, key=lambda x: abs(x[0] - peak_idx))
                persistence += abs(peak_val - closest_valley[1])
        return persistence / (len(peaks) + 1e-6)

    def _calculate_mutual_info(self) -> float:
        """Using joint histogram."""
        hist_2d = self._compute_histogram(
            torch.stack([self.current_cache["flat"], self.cross_cache["flat"]], dim=1)
        )
        h1 = self._calculate_entropy()
        self.current_cache, self.cross_cache = self.cross_cache, self.current_cache
        h2 = self._calculate_entropy()
        self.current_cache, self.cross_cache = self.cross_cache, self.current_cache
        h12 = float(-torch.sum(hist_2d * torch.log2(hist_2d + 1e-12)))
        return h1 + h2 - h12

    def _calculate_cosine_sim(self) -> float:
        """Direct cosine similarity."""
        return float(
            F.cosine_similarity(
                self.current_cache["flat"], self.cross_cache["flat"], dim=0
            )
        )

    def _calculate_cvd(self) -> float:
        """Cramer-von Mises with cached ranks."""
        xr = self.current_cache["ranks"]
        yr = self.cross_cache["ranks"]
        combined_values = torch.cat([xr, yr]).unique(sorted=True)
        cdf_x = torch.searchsorted(xr, combined_values, right=True).float() / len(xr)
        cdf_y = torch.searchsorted(yr, combined_values, right=True).float() / len(yr)
        return float((cdf_x - cdf_y).square().sum()) 

    def _calculate_cucconi(self) -> float:
        """Cucconi statistic using cached ranks."""
        xr = self.current_cache["ranks"]
        yr = self.cross_cache["ranks"]
        n, m = len(xr), len(yr)
        N = n + m
        combined = torch.cat([xr, yr])
        R = combined.argsort().argsort()[:n].float() + 1
        j = torch.arange(1, n + 1, device=self.device, dtype=torch.float32)
        j_term = (j / (n + 1) - 0.5).square()
        U_term = torch.sub(float(N + 1), R).square()
        V_term = R.square()
        U = torch.sum(U_term * j_term)
        V = torch.sum(V_term * j_term)
        N_squared_minus_4 = N * N - 4
        rho = 2 * N_squared_minus_4 / (5 * N_squared_minus_4 + 16)
        rho_term = 1 - rho * rho
        return float((U + V - 2 * rho * torch.sqrt(U * V)) / (2 * rho_term))

    def _calculate_earth_mover(self) -> float:
        """EMD using cached sorted values."""
        a_sorted = self.current_cache["sorted"]
        b_sorted = self.cross_cache["sorted"]
        if len(a_sorted) != len(b_sorted):
            if len(a_sorted) > len(b_sorted):
                indices = torch.linspace(0, len(b_sorted) - 1, len(a_sorted))
                b_sorted = torch.tensor([b_sorted[int(i)] for i in indices])
            else:
                indices = torch.linspace(0, len(a_sorted) - 1, len(b_sorted))
                a_sorted = torch.tensor([a_sorted[int(i)] for i in indices])
        return float(torch.abs(a_sorted - b_sorted).mean())

    def _calculate_distribution_overlap(self) -> float:
        """Histogram intersection from cached histograms."""
        return float(
            torch.minimum(self.current_cache["hist"], self.cross_cache["hist"]).sum()
        )


class ModelProcessor:
    """Processes ANY model architecture because tensors are tensors, baby."""

    def __init__(
        self,
        model_loader: ModelLoader,
        tensor_analyzer: TensorAnalyzer,
        database: ModelDatabase,
    ):
        self.model_loader = model_loader
        self.tensor_analyzer = tensor_analyzer
        self.database = database

    def process_and_store_model(
        self,
        model_name: str,
        model_path: str,
    ) -> None:
        """Process ANY model's tensors - architecture is just details."""
        model_id = str(uuid.uuid4())
        model_configs = self._load_model_configs(model_path)
        self.database.store_model_data(model_id, model_name, model_configs)

        model = self.model_loader.load_model(model_path)
        with sqlite3.connect(self.database.db_path) as conn:
            cursor = conn.cursor()
            for tensor_path, tensor in self._iter_model_tensors(model):
                self._process_tensor(cursor, model_id, tensor_path, tensor)

            conn.commit()

    def _iter_model_tensors(
        self, model: nn.Module
    ) -> Iterator[Tuple[str, torch.Tensor]]:
        """Iterate through ALL tensors because discrimination is bad."""
        for name, param in model.named_parameters():
            if param.requires_grad:  # Only care about trainable tensors
                yield name, param.data.clone().cpu()

    def _process_tensor(
        self,
        cursor: sqlite3.Cursor,
        model_id: str,
        tensor_path: str,
        tensor: torch.Tensor,
    ) -> None:
        """Process a tensor without judgement about its role in life."""
        tensor_id = str(uuid.uuid4())
        related_tensors = self._get_related_tensors(tensor_path, tensor)
        normalized_tensor = self.tensor_analyzer._normalize_tensor(tensor)
        analysis_results = self.tensor_analyzer.analyze_tensor(
            normalized_tensor, related_tensors
        )
        self.database.store_tensor(cursor, tensor_id, model_id, tensor_path, tensor)
        self.database.store_tensor_metrics(
            cursor,
            tensor_id,
            analysis_results["single_metrics"],
            analysis_results.get("cross_metrics"),
        )

    def _get_related_tensors(
        self, tensor_path: str, tensor: torch.Tensor
    ) -> List[torch.Tensor]:
        """Find tensors that might have meaningful relationships with this one."""
        # TODO: Add relationship detection, just return empty list fo
        return []

    def _load_model_configs(self, model_path: str) -> dict[str, str]:
        """Grab all the config files we might need later."""
        model_path = Path(model_path)
        configs = {}

        for filename in [
            "config.json",
            "tokenizer.json",
            "tokenizer_config.json",
            "special_tokens_map.json",
            "vocab.json",
            "generation_config.json",
            "added_tokens.json",
        ]:
            filepath = model_path / filename
            if filepath.exists():
                try:
                    with open(filepath, "r", encoding="utf-8") as f:
                        configs[filename.replace(".json", "_json")] = f.read()
                except Exception as e:
                    logger.error(f"Error loading config file {filename}: {e}")
                    configs[filename.replace(".json", "_json")] = None
            else:
                configs[filename.replace(".json", "_json")] = None

        return configs


class ModelLoaderFromDatabase:
    """Reassembles models from their tensor essence."""

    def __init__(self, database_dir: str):
        self.database = ModelDatabase(database_dir)
        self.temp_dir = Path(database_dir) / "temp_model_files"
        self.temp_dir.mkdir(parents=True, exist_ok=True)

    def get_model_from_db(
        self, model_name: str, device: str = "cpu", strict_loading: bool = True
    ) -> Optional[AutoModelForCausalLM]:
        """Reassembles a model from its constituent tensors."""
        if not self._prepare_model_files(model_name):
            return None
        try:
            model = AutoModelForCausalLM.from_pretrained(
                str(self.temp_dir),
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                device_map={"": device},
            )
            tensors_data = self.database.load_tensors_and_metrics(model_name)
            missing_tensors = []
            unexpected_tensors = []
            expected_params = set(n for n, _ in model.named_parameters())
            available_tensors = set(tensors_data.keys())
            missing_tensors = expected_params - available_tensors
            unexpected_tensors = available_tensors - expected_params
            if missing_tensors and strict_loading:
                raise ValueError(f"Missing tensors for {model_name}: {missing_tensors}")
            if unexpected_tensors:
                logger.warning(
                    f"Found unexpected tensors in database: {unexpected_tensors}"
                )

            for tensor_path, tensor_data in tensors_data.items():
                if tensor_path in expected_params:
                    param = model.get_parameter(tensor_path)
                    if param is not None:
                        tensor = tensor_data["tensor"].to(device)
                        if param.shape == tensor.shape:
                            param.copy_(tensor)
                        else:
                            raise ValueError(
                                f"Shape mismatch for {tensor_path}: "
                                f"expected {param.shape}, got {tensor.shape}"
                            )
            return model
        except Exception as e:
            logger.error(f"Failed to load model {model_name}: {e}")
            raise

    def get_tokenizer_from_db(
        self, model_name: str, required_files: Optional[List[str]] = None
    ) -> Optional[AutoTokenizer]:
        """Loads a tokenizer with validation."""
        required_files = required_files or ["tokenizer_config", "tokenizer"]

        if not self._prepare_model_files(model_name, required_files):
            return None
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                str(self.temp_dir), trust_remote_code=True
            )
            return tokenizer
        except Exception as e:
            logger.error(f"Failed to load tokenizer for {model_name}: {e}")
            return None

    def _prepare_model_files(
        self, model_name: str, required_files: Optional[List[str]] = None
    ) -> bool:
        """Sets up necessary files with validation."""
        self._clear_temp_directory()

        try:
            model_data = self.database.load_model_data(model_name)
            required_files = required_files or ["config"]
            for key in required_files:
                data = model_data.get(key)
                if not data:
                    raise ValueError(f"Missing required file: {key}")
                file_path = self.temp_dir / f"{key}.json"
                with open(file_path, "w") as f:
                    f.write(data)
            return True

        except Exception as e:
            logger.error(f"Failed to prepare files for {model_name}: {e}")
            return False

    def _clear_temp_directory(self) -> None:
        """Keeps our workspace clean."""
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)
        self.temp_dir.mkdir(parents=True)

    def validate_model_tensors(
        self, model_name: str, check_metrics: bool = True
    ) -> Dict[str, Any]:
        """Validates tensor consistency and optionally checks metric sanity."""
        validation_results = {
            "missing_tensors": [],
            "shape_mismatches": [],
            "metric_anomalies": [],
            "is_valid": False,
        }

        try:
            self._prepare_model_files(model_name)
            base_model = AutoModelForCausalLM.from_config(str(self.temp_dir))
            expected_params = dict(base_model.named_parameters())
            tensors_data = self.database.load_tensors_and_metrics(model_name)
            for param_name, param in expected_params.items():
                if param_name not in tensors_data:
                    validation_results["missing_tensors"].append(param_name)
                else:
                    stored_tensor = tensors_data[param_name]["tensor"]
                    if stored_tensor.shape != param.shape:
                        validation_results["shape_mismatches"].append(
                            {
                                "param": param_name,
                                "expected": param.shape,
                                "got": stored_tensor.shape,
                            }
                        )
            if check_metrics:
                for param_name, tensor_data in tensors_data.items():
                    metrics = tensor_data.get("metrics", {})
                    for metric_name, value in metrics.items():
                        if not self._is_metric_sane(metric_name, value):
                            validation_results["metric_anomalies"].append(
                                {
                                    "param": param_name,
                                    "metric": metric_name,
                                    "value": value,
                                }
                            )
            validation_results["is_valid"] = (
                not validation_results["missing_tensors"]
                and not validation_results["shape_mismatches"]
                and (not check_metrics or not validation_results["metric_anomalies"])
            )
            return validation_results

        except Exception as e:
            logger.error(f"Validation failed for {model_name}: {e}")
            validation_results["error"] = str(e)
            return validation_results


class MergeReportManager:
    """Handles the sacred texts of model merging."""

    def __init__(self, database: ModelDatabase):
        self.database = database
        self._init_merge_tables()

    def _init_merge_tables(self):
        """Set up our merge report shrine."""
        with sqlite3.connect(self.database.db_path) as conn:
            cursor = conn.cursor()
            cursor.executescript("""
                -- The sacred scrolls of model merging
                CREATE TABLE IF NOT EXISTS merge_reports (
                    report_id TEXT PRIMARY KEY,
                    base_model_name TEXT NOT NULL,
                    creation_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    config_json TEXT,  -- Keep the recipe
                    metrics_json TEXT  -- How well did it work?
                );
                
                -- Which tensors came from where
                CREATE TABLE IF NOT EXISTS merge_tensor_sources (
                    report_id TEXT NOT NULL,
                    tensor_path TEXT NOT NULL,  -- Where in the model
                    source_model TEXT NOT NULL,  -- Where we got it from
                    metrics_json TEXT,  -- How good was this choice?
                    FOREIGN KEY(report_id) REFERENCES merge_reports(report_id),
                    UNIQUE(report_id, tensor_path)
                );
            """)

    def save_merge_report(self, merge_report: Dict) -> str:
        """Preserve the sacred knowledge of how we built this monster."""
        report_id = str(uuid.uuid4())

        with sqlite3.connect(self.database.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO merge_reports (
                    report_id, base_model_name, config_json, metrics_json
                ) VALUES (?, ?, ?, ?)
                """,
                (
                    report_id,
                    merge_report["base_model"]["name"],
                    json.dumps(merge_report.get("config", {})),
                    json.dumps(merge_report["base_model"].get("metrics", {})),
                ),
            )

            tensor_sources = []
            if "boundary_layers" in merge_report:
                boundary = merge_report["boundary_layers"]
                if boundary.get("name"):
                    tensor_sources.extend(
                        [
                            (
                                report_id,
                                "model.embed_tokens",
                                boundary["name"],
                                json.dumps(boundary.get("metrics", {})),
                            ),
                            (
                                report_id,
                                "model.norm",
                                boundary["name"],
                                json.dumps(boundary.get("metrics", {})),
                            ),
                            (
                                report_id,
                                "lm_head",
                                boundary["name"],
                                json.dumps(boundary.get("metrics", {})),
                            ),
                        ]
                    )

            if "layers" in merge_report:
                for layer_idx, layer_info in merge_report["layers"].items():
                    source_model = layer_info.get("best_model")
                    if source_model:
                        tensor_sources.extend(
                            [
                                (
                                    report_id,
                                    f"model.layers.{layer_idx}",
                                    source_model,
                                    json.dumps(layer_info.get("metrics", {})),
                                )
                            ]
                        )

            cursor.executemany(
                """
                INSERT INTO merge_tensor_sources (
                    report_id, tensor_path, source_model, metrics_json
                ) VALUES (?, ?, ?, ?)
                """,
                tensor_sources,
            )

            conn.commit()

        return report_id

    def load_merge_report(self, report_id: str) -> Dict:
        """Resurrect the ancient knowledge."""
        with sqlite3.connect(self.database.db_path) as conn:
            cursor = conn.cursor()

            cursor.execute(
                """
                SELECT base_model_name, config_json, metrics_json
                FROM merge_reports WHERE report_id = ?
                """,
                (report_id,),
            )

            result = cursor.fetchone()
            if not result:
                raise ValueError(f"No merge report found for ID {report_id}")

            base_model, config_json, metrics_json = result

            report = {
                "base_model": {
                    "name": base_model,
                    "metrics": json.loads(metrics_json) if metrics_json else {},
                },
                "config": json.loads(config_json) if config_json else {},
            }

            cursor.execute(
                """
                SELECT tensor_path, source_model, metrics_json
                FROM merge_tensor_sources
                WHERE report_id = ?
                ORDER BY tensor_path
                """,
                (report_id,),
            )
            for path, source, metrics in cursor.fetchall():
                metrics = json.loads(metrics) if metrics else {}
                if path in ["model.embed_tokens", "model.norm", "lm_head"]:
                    if "boundary_layers" not in report:
                        report["boundary_layers"] = {"name": source, "metrics": metrics}
                elif path.startswith("model.layers."):
                    layer_idx = path.split(".")[2]
                    if "layers" not in report:
                        report["layers"] = {}
                    report["layers"][layer_idx] = {
                        "best_model": source,
                        "metrics": metrics,
                    }
            return report

    def get_model_from_report(
        self, report_id: str, device: str = "cpu"
    ) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
        """Resurrect the monster from our database."""
        report = self.load_merge_report(report_id)
        base_model_name = report["base_model"]["name"]
        model = self.database.load_model(base_model_name, device)
        tokenizer = self.database.load_tokenizer(base_model_name)

        if model is None or tokenizer is None:
            raise ValueError(f"Failed to load base model {base_model_name}")

        if "boundary_layers" in report and report["boundary_layers"]["name"]:
            try:
                boundary_model = report["boundary_layers"]["name"]
                boundary_tensors = self.database.load_tensors(boundary_model)

                for tensor_name in ["model.embed_tokens", "model.norm", "lm_head"]:
                    if tensor_name in boundary_tensors:
                        self._set_tensor(
                            model, tensor_name, boundary_tensors[tensor_name]
                        )
            except Exception as e:
                logger.error(f"Failed to apply boundary layers: {e}")

        if "layers" in report:
            for layer_idx, layer_info in report["layers"].items():
                source_model = layer_info["best_model"]
                try:
                    layer_tensors = self.database.load_tensors(
                        source_model, f"model.layers.{layer_idx}"
                    )
                    for tensor_path, tensor in layer_tensors.items():
                        self._set_tensor(model, tensor_path, tensor)
                except Exception as e:
                    logger.error(f"Failed to apply layer {layer_idx}: {e}")

        return model, tokenizer

    @staticmethod
    def _set_tensor(model: nn.Module, tensor_path: str, tensor: torch.Tensor):
        """Carefully place tensor in its new home."""
        try:
            module_path, param_name = tensor_path.rsplit(".", 1)
            if module_path:
                module = model.get_submodule(module_path)
            else:
                module = model
            param = getattr(module, param_name)
            with torch.no_grad():
                param.copy_(tensor)
        except Exception as e:
            raise ValueError(f"Failed to set tensor {tensor_path}: {e}")


def load_config(config_path: str) -> dict:
    """Loads configuration from a YAML file."""
    try:
        with open(config_path, "r") as file:
            return yaml.safe_load(file)
    except Exception as e:
        logger.error(f"Error loading config from {config_path}: {e}")
        raise


def create_conversation_signatures(qa_pairs, system_prompts):
    documents = []
    for (q, a), system_prompt in zip(qa_pairs, system_prompts):
        system_freq = Counter(system_prompt.split())
        question_freq = Counter(q.split())
        answer_freq = Counter(a.split())
        combined_text = f"{system_prompt} {q} {a}".split()
        total_freq = Counter(combined_text)

        documents.append(
            {
                "system_prompt": system_prompt,
                "human": q,
                "gpt": a,
                "term_frequencies": {
                    "system": dict(system_freq),
                    "question": dict(question_freq),
                    "answer": dict(answer_freq),
                    "combined": dict(total_freq),
                },
                "document_length": len(combined_text),
                "section_lengths": {
                    "system": len(system_prompt.split()),
                    "question": len(q.split()),
                    "answer": len(a.split()),
                },
            }
        )
    return documents


@dataclass
class RankBasedTensorScore:
    tensor_name: str
    ranks: Dict[str, int]  # Dataset -> rank mapping
    signature_ranks: Dict[str, int]  # Signature -> rank mapping

    @property
    def aggregate_rank(self) -> float:
        all_ranks = list(self.ranks.values()) + list(self.signature_ranks.values())
        if not all_ranks:
            return float("inf")
        return np.exp(np.mean(np.log(all_ranks)))


class TensorRankAggregator:
    def __init__(self, metric_names: List[str]):
        self.metric_names = metric_names
        self.signature_cache = {}

    def compute_ranks(
        self,
        tensors: Dict[str, Dict[str, float]],
        dataset_metrics: Dict[str, Dict[str, float]],
        signatures: Optional[List[Dict]] = None,
    ) -> List[RankBasedTensorScore]:
        tensor_scores = []

        for tensor_name, tensor_metrics in tensors.items():
            dataset_ranks = {}
            for dataset, scores in dataset_metrics.items():
                valid_scores = {t: s for t, s in scores.items() if t in tensors}
                ranks = rankdata([s for s in valid_scores.values()])
                tensor_idx = list(valid_scores.keys()).index(tensor_name)
                dataset_ranks[dataset] = ranks[tensor_idx]

            signature_ranks = {}
            if signatures:
                signature_ranks = self._compute_signature_ranks(
                    tensor_name, tensor_metrics, signatures
                )

            tensor_scores.append(
                RankBasedTensorScore(
                    tensor_name=tensor_name,
                    ranks=dataset_ranks,
                    signature_ranks=signature_ranks,
                )
            )

        return sorted(tensor_scores, key=lambda x: x.aggregate_rank)

    def _compute_signature_ranks(
        self, tensor_name: str, tensor_metrics: Dict[str, float], signatures: List[Dict]
    ) -> Dict[str, int]:
        """Compute ranks based on signature similarity and performance."""
        signature_scores = {}
        for sig_idx, signature in enumerate(signatures):
            sig_key = f"signature_{sig_idx}"
            if sig_key not in self.signature_cache:
                self.signature_cache[sig_key] = self._extract_signature_metrics(
                    signature
                )
            sig_metrics = self.signature_cache[sig_key]
            match_score = self._compute_metric_similarity(tensor_metrics, sig_metrics)
            signature_scores[sig_key] = match_score

        ranks = rankdata(list(signature_scores.values()))
        return {k: int(r) for k, r in zip(signature_scores.keys(), ranks)}

    def _extract_signature_metrics(self, signature: Dict) -> Dict[str, float]:
        """Extract relevant metrics from a conversation signature."""
        metrics = {}
        term_freqs = signature["term_frequencies"]["combined"]
        total_terms = sum(term_freqs.values())
        metrics["entropy"] = -sum(
            (f / total_terms) * np.log2(f / total_terms) for f in term_freqs.values()
        )
        metrics["sparsity"] = len(term_freqs) / total_terms
        metrics["avg_freq"] = total_terms / len(term_freqs) if term_freqs else 0
        return metrics

    def _compute_metric_similarity(
        self, tensor_metrics: Dict[str, float], signature_metrics: Dict[str, float]
    ) -> float:
        """Compute similarity between tensor metrics and signature metrics."""
        comparable_metrics = ["entropy", "sparsity"]
        similarity = 0.0
        for metric in comparable_metrics:
            if metric in tensor_metrics and metric in signature_metrics:
                diff = abs(tensor_metrics[metric] - signature_metrics[metric])
                max_val = max(
                    abs(tensor_metrics[metric]), abs(signature_metrics[metric])
                )
                if max_val > 0:
                    similarity += 1 - (diff / max_val)

        return similarity / len(comparable_metrics) if comparable_metrics else 0.0

    def select_top_tensors(
        self,
        tensor_scores: List[RankBasedTensorScore],
        n: int = 3,
        diversity_threshold: float = 0.3,
    ) -> List[str]:
        """Select top tensors while ensuring diversity."""
        if not tensor_scores:
            return []

        selected = [tensor_scores[0].tensor_name]
        remaining = tensor_scores[1:]

        while len(selected) < n and remaining:
            for i, score in enumerate(remaining):
                is_diverse = True
                for prev_tensor in selected:
                    prev_score = next(
                        s for s in tensor_scores if s.tensor_name == prev_tensor
                    )
                    rank_correlation = self._compute_rank_correlation(
                        score.ranks, prev_score.ranks
                    )

                    if rank_correlation > (1 - diversity_threshold):
                        is_diverse = False
                        break

                if is_diverse:
                    selected.append(score.tensor_name)
                    remaining.pop(i)
                    break
            else:
                selected.append(remaining[0].tensor_name)
                remaining = remaining[1:]

        return selected

    def _compute_rank_correlation(
        self, ranks1: Dict[str, int], ranks2: Dict[str, int]
    ) -> float:
        """Compute correlation between two sets of ranks."""
        common_keys = set(ranks1.keys()) & set(ranks2.keys())
        if not common_keys:
            return 0.0
        x = [ranks1[k] for k in common_keys]
        y = [ranks2[k] for k in common_keys]
        return np.corrcoef(rankdata(x), rankdata(y))[0, 1]


@torch.inference_mode()
def main(config_path: str):
    """Main function to run the model analysis process."""
    config = load_config(config_path)
    database_dir = config["database_dir"]
    models_dir = config["models_dir"]
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Using device: {device} for metric calculations")
    os.makedirs(models_dir, exist_ok=True)
    database = ModelDatabase(database_dir)
    model_loader = ModelLoader(models_dir)
    tensor_analyzer = TensorAnalyzer(device)
    model_processor = ModelProcessor(model_loader, tensor_analyzer, database)
    models = [config["base_model"]] + config["fine_tuned_models"]
    for model_name in models:
        try:
            local_model_path = model_loader.ensure_model_files(model_name)
            model_processor.process_and_store_model(model_name, local_model_path)
        except Exception as e:
            logger.error(f"Failed to process model {model_name}: {e}")

    logger.info("Model processing and metric calculation complete.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Simplified model management and analysis tool"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="mastermerge_config.yaml",
        help="Path to configuration file",
    )
    args = parser.parse_args()
    main(args.config)
